[{"content":"I\u0026rsquo;m a Data Scientist with a Master\u0026rsquo;s degree in Data Science, currently perusing Freelance Data Science Projects.\nIn my spare time, I write blogs about data science, machine learning, finance and career advice.\n","date":null,"permalink":"/","section":"About","summary":"","title":"About"},{"content":"","date":null,"permalink":"/blog/","section":"Blogs","summary":"","title":"Blogs"},{"content":"","date":null,"permalink":"/tags/computer-science/","section":"Tags","summary":"","title":"Computer Science"},{"content":"","date":null,"permalink":"/tags/data-analysis/","section":"Tags","summary":"","title":"Data Analysis"},{"content":"","date":null,"permalink":"/tags/data-science/","section":"Tags","summary":"","title":"Data Science"},{"content":"","date":null,"permalink":"/tags/linear-regression/","section":"Tags","summary":"","title":"Linear Regression"},{"content":" Linear regression models are a fundamental topic in data science that falls into the category of machine learning. This type of machine-learning model is an incredibly fundamental model that is used very regularly through a variety of fields and has multiple applications, which can also be of a similar nature.\nFor example, the model can be used for either data prediction or data analysis, and given the breadth of these fields, we will be able to observe how incredibly useful and relevant this type of model can be to aid businesses with business insights or help forecast potential shortcoming or downturns of any industries.\nLinear regression models generate huge amounts of value for a plethora of industries through the extraction of valuable insights that could save businesses millions of pounds, save businesses from the consequences of an economic recession, or help inform large-scale decision-making for heads of departments.\nIn this article, we shall be exploring the inner workings of linear regression models, how to create them, and provide you, as upcoming data scientists with the relevant knowledge you need to have a good understanding of how and why these models are important for all walks of life.\nWhat do I need to know about linear regression? #Linear regression is a form of machine learning model that is used for the purposes of statistical analysis and data analysis and allows us to model the relationship between two variables, known as the independent and dependent variable. This can be shown mathematically and through the use of coding. However, it is essential to understand the mathematics behind such models before delving deeper.\nLinear regression can be displayed mathematically with the following notations:\nThis is the formula for a linear equation: ( y = mx + c )\nY is referred to as the dependent variable. This is what we‚Äôre trying to predict. M represents the slope of the linear regression line in the model. Used to observe how much y will change when x changes by 1 unit. B represents the intercept. This will show us what our starting prediction will be when x = 0. X represents the independent variable. This is used to show how x impacts our prediction of y and their relationship. When we take the values above and put them together inside a model what we then have is the linear regression model. This is known as a simple linear regression model because it models the relationship between one dependent variable and one independent variable.\nHowever, we can also have what‚Äôs known as a multiple linear regression model. However, this is usually done when we are trying to examine and model the relationship between the dependent variable and multiple independent variables.\nUltimately, when examining the dataset you will be able to determine if the most appropriate will either be a simple linear regression model or multiple linear regression model.\nThis is because multiple linear regression models are usually more appropriate when you are provided with a dataset that has many variables that seem important and relevant to helping predict what the dependent variable is, and what the relationship between the dependent variable and multiple independent variables is.\nWhat are some real-world applications of linear regression models? #Linear regression models have many real-world applications. This is because any field that requires some degree of prediction and those fields can be within any of the fields listed below:\nFinance Economics Healthcare Real estate Sports analytics Energy and environment A prime example of this could be Amazon attempting to predict what its future stock price is going to be. Therefore, the data analysts and data scientists at Amazon may attempt to use all the relevant available data and create a linear regression model to identify the relationships between different variables and stock prices.\nOther factors included as independent variables could be variables such as seasonality, technological progress, brand image, and profits. This is because these factors may have some relationship with stock price and so it may be best to attempt to identify if a relationship is present and predict future stock prices based on these variables.\nThese types of models form the fundamental foundation for a whole variety of other algorithms because usually linear regressions or multiple linear regression models will be used to identify any relationships that are present between variables in the data and the further more complex machine learning algorithms will be deployed to extract deeper insights into the inner working of the dataset provided.\nBeing able to have ability to reflect on your findings for applying a simple linear regression or multiple linear regression will allow one to observe whether the increased complexity in the new models such as neural network models or logistic regression is justified in its efforts. Given that these models usually will attempt to build further upon the foundation of a linear regression model, with added complexity.\nAssumptions of the linear regression model: #Linear regression models also appear to have assumptions being made about either the relationships between variables inside the dataset or the data itself.\nThese assumptions are:\nLinearity between the variables. This means assuming there is a proportional change in Y for every change in X. Independence. This means that each piece of data should not be able to influence the others. Homoscedasticity. This means the variance of prediction errors should remain constant across different values of X. Normality. This would mean that the errors of the model should follow a normal distribution. Therefore, any one of these issues could be violated by either the dataset or the relationship that is present between the variables. Financial datasets very oftenly violate the assumptions of independence because variables such as stock prices are influenced by their past values.\nThus, the most appropriate machine learning model to deploy depends on the assumptions of the model not being violated before deployment through thorough analysis of the nature of the dataset a Data Scientist must choose the model they deem most suitable for the job.\nSummary: #It is evident that linear regression models are a foundational machine learning model used throughout many domains of data science and data analytics, as these models can provide the foundation for more complex models and serve as a means of comparison. But, whether these models will be appropriate for use will depend on if the assumptions of the models are upheld or violated, as this will allow one to determine if this is the appropriate model to use to identify relationships between the variables and make predicts about the dependent variable, or not.\nAlso! #If you enjoyed this article, please feel free to read my other articles where I regularly post about new data science topics and content to help inform you of the latest data science trends and foundational topics.\nHave a great week ahead! üëã\n","date":"22 February 2025","permalink":"/blog/linear-regression-explained/","section":"Blogs","summary":"","title":"Linear Regression for Machine Learning Explained"},{"content":"","date":null,"permalink":"/tags/machine-learning/","section":"Tags","summary":"","title":"Machine Learning"},{"content":"","date":null,"permalink":"/tags/","section":"Tags","summary":"","title":"Tags"},{"content":"","date":null,"permalink":"/tags/artificial-intelligence/","section":"Tags","summary":"","title":"Artificial Intelligence"},{"content":"","date":null,"permalink":"/tags/fraud-detection/","section":"Tags","summary":"","title":"Fraud Detection"},{"content":" How common is fraud? #Fraud is prevalent throughout society with common news articles showing up on the BBC such as ‚ÄúVictims lose ¬£7m to romance fraud in one year‚Äù (Sinclair, 2025). Or, ‚ÄúGhost broking: Young and vulnerable people targeted by insurance scam‚Äù (Pandey, 2022).\nFraud runs rampant throughout our society, yet it appears that it falls through the cracks of our financial fraud detection systems, commonly being detected one step too late.\nThe impacts of fraud can be disastrous for all of those who have had the unfortunate luck of dealing with fraudsters. Fraud can easily cost companies millions of pounds and can cause pensioners to lose all their savings which they have spent decades accumulating. This can be observed as the approximate cost of fraud alone is equivalent to ¬£6.8 billion in the United Kingdom and Wales (Cleverly \u0026amp; Tugendhat, 2024).\nStatistics such as these made me interested in understanding what types of fraud take place within society and made me ponder what can be done to minimise it and or ideally negate it. Hence, I took it upon myself to try and research such issues further to investigate what the data science solutions would be to resolve such issues, as data is a powerful tool when used in the correct hands, so I wished to analyse fraudulent transaction-based data and observe how I could deploy my knowledge of machine learning to help combat a very prevalent real-world finance issue.\nIn this article, I shall be exploring and discussing the machine learning (ML) methods I deployed for fraudulent transaction detection the issues surrounding detecting fraudulent transactions when deploying machine learning models for such purposes, and any general insights I may have in relation to such topics to share my understanding of such topics.\nWhat are the different types of fraud? #Fraud can be defined as ‚Äúcriminal deception intended to result in personal or financial gain‚Äù. I believe this definition encompasses the different types of fraud that we may be discussing throughout this article, which can take the form of:\nInsurance fraud Pension fraud. E-commerce fraud. Credit card fraud. Mobile fraudulent transactions. Romance fraud. Fraud is a vast domain for criminals to tap into for significant financial gain, and fraudulent behaviour may come from one individual who is running their operations, or a group of individuals working with one another to ensure that the scam runs as smoothly as possible to minimise the probability of detection and possible criminal charges being pressed against them.\nCredit card fraud alone is a prominent cause of concern for the banking industry and global financial health. In the Single Euro Payment area, we can observe that credit card fraud amounted to 1.8 billion Euros in 2019 and the total loss from credit card fraud amounted to $21.84 billion, which was observed globally that year (Raghavan \u0026amp; Gayar, 2019).\nCredit card fraud is a transactional type of fraud that can also occur within seconds, as fraudsters are adapting to common fraud prevention strategies deployed by financial institutions, and although financial institutions are adapting their strategies to attempt to keep up with fraudsters sometimes this alone is not enough, as displayed by our statistics above.\nCredit card fraud also has many forms and can fall into multiple domains such as:\nAccount takeover. Application fraud. Lost or stolen credit card. Non-received items fraud. Why is it so difficult to simply stop these fraudsters? #The primary challenge in fraud prevention is fraud detection; without it, fraud goes unnoticed and fraudsters will be able to steal the information or data they need to pull off their quick heist and reap the financial rewards from such lucrative pursuits.\nData scientists find it especially difficult to get access to significant quantities of sensitive, personal financial data to analyse and deploy the most efficient or effective machine learning (ML) fraud detection methods.\nFinancial data contains a lot of sensitive information, and using such data would be very helpful in creating machine learning models for credit card fraud detection. However, compliance rules must be maintained in a skilled manner. Also, areas such as GDPR, and data privacy must be respected, and consent must be provided for such information to be used for the training and development of machine learning models. These policies make fraudulent transaction detection a highly sensitive and restricted field, limiting data scientists‚Äô access to crucial data and potentially limiting ML model deployment and development.\nIn addition, security concerns play a significant role in handling such sensitive information. This is because financial data may contain very personal data about individuals‚Äô date of birth, income, address, marital status, mortgage payments, and more. Therefore, access remains extremely limited to maintain privacy and this is an area data scientists must respect and work around, but this is an area that does add an element of difficulty to such ML modelling tasks.\nFurthermore, fraud detection datasets appear to be extremely imbalanced which adds to the tasks of data scientists. This dataset will usually contain a significant amount of non-fraudulent transactions and an infinitesimal proportion of fraudulent transactions, as they‚Äôre sprinkled throughout a dataset which can sometimes contain millions of transactions or more. Therefore, data scientists must utilise effective methods for balancing the data before they attempt to detect and minimise fraud.\nWhat is machine learning and how is it used for fraud detection? #Machine learning revolves around exposing machines to human data, which over time will allow the machine to detect patterns, and learn the way that humans learn, adapt, and evolve their fraudulent behaviours all of which is usually done through the use of artificial intelligence (IBM, 2025).\nMachine learning is a common tool used by data scientists across a variety of industries to resolve fraud detection and to aid with fraud prevention, and some of the ML models that typically tend to be used for such measures are:\nLogistic regression. Random forest. Artificial neural networks (ANNs). K-means clustering. XGBoost. Support vector machines (SVMs). K-nearest neighbours. Convolutional neural networks (CNNs) Machine learning models can also fall into the categories of supervised and unsupervised learning models.\nUnsupervised learning #Unsupervised machine learning models can identify patterns and structures within data without being provided with a focus variable, unlike supervised machine learning, which requires labelled data. These models do not need labelled data because the algorithms search for shared characteristics between instances or patterns within the data, proceeding to cluster and group them accordingly.\nUnsupervised machine learning techniques are commonly used in fraud detection, product segmentation, image recognition, scientific discovery, and movie recommendations on streaming platforms like Netflix.\nSome of the ML models that typically fall into this category are:\nK-means clustering Principal component analysis (PCA) Autoencoders Supervised learning #This type of machine learning is defined by the use of input and output variables. Within this, the model is trained using input data and output data, and it works through the use of labelled data, which is data that contains features(input) and targets (output). The models deployed attempt to understand the relationship between the input data that is labelled and the output data used to train.\nThis allows the model to provide predictions or classifications when given unseen instances.\nML models that fall into this category will typically be:\nXGBoost Logistic regression Random forest Linear regression Support vector machines (SVMs) Decision trees Overall, models used for ML will be used for classification or regression challenges.\nFraud detection and prevention in nature would be considered to be a classification problem that is being attempted to be solved through the use of ML models. Therefore, it‚Äôs only appropriate we explain this below:\nClassification problem #Classification problems will range around issues such as fraud detection because as a Data Scientist, you will be provided with significant amounts of data that haven‚Äôt been placed into any category and therefore require an ML model to do this for you. E.g. Fraudulent transaction data will have two types of data referred to as the majority class (legitimate transactions) and the minority class (fraudulent transactions). Hence, it will be the ML model‚Äôs job to predict the likelihood of a transaction being fraudulent based on trends and patterns shown to it.\nResults #Although I ran tests on my dataset using XGBoost, random forest, logistic regression, and k-means clustering. The results displayed the XGBoost model as the most effective at fraud transaction detection.\nOne of the most effective models that were shown throughout the literature was XGBoost and through my findings, I was also able to replicate this with my original findings.\nI then attempted to further optimise the model through the use of hyperparameter optimisation, which is a series of tweaks that an individual can make for each of their ML models to further improve their performance and accuracy when it comes to fraud detection.\nConclusion: #Overall, incentives for fraud have never been higher and so fraudsters will continue to innovate and adapt their fraudulent transaction to overcome typical fraud detection methods used by industries such as finance. However, finance industries simultaneously have been attempting to invest heavily in research and development to counter the attempts of fraudsters and detect fraud to protect their businesses and ensure the safety of their customers‚Äô transactions.\nML models for fraudulent transaction detection appear to be improving with the significant improvements in artificial intelligence, hardware capabilities, and significantly greater supplies of highly skilled data scientists in the workplace to help thwart such fraudulent behaviour.\nFrom my research and observations, XGBoost appears to be incredibly effective at detecting fraudulent transactions with the random forest model being a close contender in second place. Both models demonstrated strong performance due to their ability to optimise a wide range of hyperparameters, which we did. However, XGBoost was the most effective for such classification tasks, due to robust default hyperparameters and built-in boosting framework built into the model.\nAlso! #If you enjoyed this article, please feel free to read my other articles where I regularly post about new data science topics and content to help inform you of the latest data science trends and foundational topics.\nHave a great week ahead! üëã\nReferences: #Cleverly, J. and Tugendhat, T. (2024) Major campaign to fight fraud launched, GOV.UK. Available at: https://www.gov.uk/government/news/major-campaign-to-fight-fraud-launched#:~:text=The%20estimated%20cost%20of%20fraud,a%20decline%20now%20being%20observed. (Accessed: 15 February 2025).\nIBM (2025) What is machine learning (ML)?, IBM. Available at: https://www.ibm.com/think/topics/machine-learning (Accessed: 15 February 2025).\nRaghavan, P. \u0026amp; Gayar, N.E., 2019. Fraud detection using machine learning and deep learning. 2019 International Conference on Computational Intelligence and Knowledge Economy (ICCIKE), Dubai, United Arab Emirates, pp. 334‚Äì339. doi:10.1109/ICCIKE47802.2019.9004231.\nPandey, M. (2022) Ghost broking: Young and vulnerable people targeted by insurance scam, BBC News. Available at: https://www.bbc.com/news/newsbeat-61992772 (Accessed: 15 February 2025).\nSinclair, E. (2025) Romance fraud: Victims in Surrey and Sussex lose ¬£7M in one year, BBC News. Available at: https://www.bbc.co.uk/news/articles/czrldx6zk8ko (Accessed: 15 February 2025).\n","date":"15 February 2025","permalink":"/blog/machine-learning-for-fraud-detection/","section":"Blogs","summary":"","title":"Machine Learning for Fraud Detection"},{"content":"","date":null,"permalink":"/tags/career-advice/","section":"Tags","summary":"","title":"Career Advice"},{"content":"","date":null,"permalink":"/tags/internship/","section":"Tags","summary":"","title":"Internship"},{"content":" Introduction: #Internships and placement years are so highly praised whilst being at university. If you‚Äôre able to land one, it‚Äôs said that it could be the magic opportunity that could set up your long-term career prospects.\nAfter countless interviews, numerical tests, and coffee breaks, I successfully landed a placement role, while I was studying economics as an Intern MI Analyst, and I was hoping to start as soon as possible in Brighton, East Sussex.\nThroughout this placement, I worked across teams, honed my analytical skills, and developed my knowledge of SQL, PowerBI, and Microsoft Excel to deliver business insights to individuals across the organisation where I worked this allowed me to understand the importance of SQL and having experience using it.\nIn this article, I will be sharing my insights relating to my placement experience and the importance of securing a placement opportunity to maximise your future employment prospects within data science. I hope that the insights I will deliver will provide a better understanding of what to expect from a data-oriented placement year and emphasize the importance of landing one for data science.\nWhat is an MI Analyst? #MI Analyst stands for Management Information Analyst. Individuals within such roles usually utilise a variety of technologies to support data-driven decision-making across the organisation that they work for.\nAnalysts usually have to communicate across the organisation to be able to ensure that they will produce the appropriate data output that is required by a range of individuals from across an organisation, and ensure that the data is easily able to be understood by almost anyone irrespective of whether they are from a numerical background or not.\nMI analysts play an essential role across a wide range of organisations and can be commonly found in industries that rely on data-driven insights and reporting.\nMI analysts can be found working within various industries and organisations such as:\nConsulting Banking and Finance Telecommunications Insurance Healthcare Retail and E-commerce Government They can generate significant amounts of value by allowing such organisations and industries to observe wherever they may have shortcomings so that they can make rapid adjustments to boost their overall productivity, profits, and total revenues.\nWhat are the primary tasks of an MI Analyst? # Data collection and reporting regularly using databases and spreadsheets. Data cleansing by removing irrelevant features from the data to later provide meaningful data insights. Producing or maintaining existing reports on a weekly, monthly, or quarterly basis. Providing performance analysis by identifying trends and analysing behaviours across time. Creating interactive dashboards in PowerBI, Tableau, or Power Query. Advising managers based on data trends and supporting their decision-making process. Therefore, it is evident that MI analysts will require the use of a variety of different technologies and usually require some combination of SQL, PowerBI, Excel, and a range of other Data Analytics and Business Intelligence tools to support data-driven decision-making and very often such analysts will be approached from a range of managers that work for different departments.\nFor example, a performance manager may schedule a meeting with you to discuss a new report that they‚Äôre interested in seeing which allows them to easily keep track of and observe the amount of hours lawyers have been working this week.\nTherefore, I would attend this meeting with the intention of specifying what the desirable outcome would look like for the involved party.\nThis would mean that I would require the following:\nWhat data visualisations would they like me to produce or modify? What is the proposed deadline for producing such a product? What are their needs and what do they require? Would they like the report to be self-adjusting or require manual adjustment? I would usually then decide the most appropriate course of action based on what data I had available to myself on SQL. This would allow me to see what data I could easily extract and modify to meet their needs e.g. If it was about producing a report to see hours worked by lawyers, I would look at the corresponding databases and extract all the relevant information using the SQL coding techniques that I have been taught by my senior colleagues.\nOnce this step had been performed, I would modify a report using PowerPoint, or PowerBI to provide this information back to the client.\nOn the other hand, the client‚Äôs request would sometimes be as simple as just having this information provided back in a raw format, such as a table, or reported verbally for note-keeping.\nThis illustrates the variety of tasks an MI Analyst may handle, as they switch from SQL to PowerBI to Excel to Microsoft PowerPoint to juggle a number of data-driven operations, but this provides a massive learning opportunity that shows you how deep the world of data can truly go.\nWhy do I need a placement year as an aspiring data scientist? #Applying to data science jobs can be difficult, especially with the incredibly competitive job market that we currently have to deal with, with many people struggling to land their first role.\nHaving valuable work experience before you graduate from university is something that can be a significant stepping stone that can launch your data science career on an upward trajectory and set you apart from the competition.\nColleagues I previously worked with deployed this strategy, as they started working as Intern MI analysts, and they were successfully able to land a part-time role as MI Analyst to then work up towards being a data scientist.\nThis is incredibly useful because you get hands-on experience in applying and dealing with vast datasets, creating data visualisations, and coding in SQL, a highly in-demand coding language, used for storing, manipulating, and retrieving data, which is commonly used for data science.\nExperience is key when it comes to applying for data science jobs, and currently, competition has never been fiercer.\nIf you\u0026rsquo;re currently at university and reading this, it\u0026rsquo;s best to try and opt for as many chances as you get to work in a data-related field, as you\u0026rsquo;ll have many new opportunities to work with different large databases and learn new skills that you otherwise may not have been able to learn.\nIf you get the opportunity, you won\u0026rsquo;t regret it.\nNetworking opportunities #Making friends within the workplace is an amazing opportunity to build lifelong connections who would likely support you throughout your data science career.\nWorking as an Intern MI Analyst, alongside senior MI analysts allowed me to learn directly from my colleagues and make important connections.\nThese connections can influence the trajectory your career spirals towards, as your colleagues can provide feedback, one-on-one support, and career advice about data science by accessing their own network and making referrals on your behalf to support your development.\nFurthermore, a major advantage of having a data-related placement is that it allows you to leverage connections for personal growth and development. However, these opportunities will only arise when the correct opportunity presents itself and you take the chance to apply for the role.\nSummary and final thoughts #Overall, securing a placement as an Intern MI Analyst is an excellent opportunity to work towards a career in data science because of the coding skills you\u0026rsquo;ll develop through extensive use of SQL and other data analytics and business intelligence tools throughout the year. However, if one considers all the networking opportunities that also arise from getting early experience this truly is a win-win scenario to help kickstart your long-term data science career.\nI\u0026rsquo;d highly recommend applying for such roles whenever they become available to you throughout your studies to boost your long-term data science career prospects.\nAlso! #If you enjoyed this article, please feel free to take a read of my other articles where I regularly post about new data science topics and content to help inform you of the latest data science trends and foundational topics.\nHave a great week ahead! üëã\n","date":"9 February 2025","permalink":"/blog/data-science-placement/","section":"Blogs","summary":"","title":"The Importance of a Placement Year for Data Science Success"},{"content":"","date":null,"permalink":"/tags/uk/","section":"Tags","summary":"","title":"UK"},{"content":"","date":null,"permalink":"/tags/university/","section":"Tags","summary":"","title":"University"},{"content":"üìá My Contact Details You can reach me through the following platforms:\nüìß Email: Business Email üìû Phone: 07366388100 üíº LinkedIn: Aamir Raja ","date":null,"permalink":"/contacts/","section":"About","summary":"","title":"Contact"},{"content":"My projects spanned across a variety of different domains and have allowed me to enhance my coding skills through use of a wide variety of different libraries.\nPython projects I\u0026rsquo;ve completed showcase my skills for:\nMachine Learning Models Exploratory Data Analysis and Data Visualisation Data Cleansing and Data Wrangling Statistical Analysis Object-Oriented Programming Algorithims Analysis Throughout this page I will explore with you some of the core projects that I completed within my Data Science MSc. However, not all of my projects are displayed here so, please click here to see more of my projects if you\u0026rsquo;re interested.\nMachine Learning for Fraudulent Transaction Detection ‚öôÔ∏è # Deployed baseline supervised and unsupervised models (XGBoost, Random forest, logistic regression, K-means clustering) enabling fraud detection with an initial F1-Score of 50-90%(depending on the model used) Compared the efficiency of models using F1-Scores, assisted by contemporary literature enhancing analysis quality by 13% Optimised Random forest and XGBoost models via GridSearchCV, leading to ¬£ savings for this potential businesses Utilised Random Undersampling to balance class distribution, improving model training times by up to 90% Performed data transformation using label encoding, leading to a 12% boost in fraud classification accuracy Performed exploratory data analysis (EDA) to uncover class imbalance, allowing for the application of data balancing techniques Conducted comparative analysis of different machine learning models, identifying the best-performing approach Libraries utilised:\nPandas Numpy Sklearn Seaborn Matplotlib Imblearn XGBoost Development environment:\nJupyter Notebook (Anaconda) Health Data Analysis üçé # Performed data cleansing and wrangling by addressing missing values, removing duplicates, and standardising formats, enhancing data quality by approximately 40%, which facilitated effective data visualisation and analysis Performed exploratory data analysis (EDA) to examine dataset structure, uncovering critical trends to support feature selection Utilised data transformation techniques, including feature engineering, to create new relevant variables, resulting in a 19% improvement in health data analysis Utilised data visualisations to identify patterns and correlations to help select appropriate features for mathematical modelling Deployed mathematical models to predict heart rate, identifying trends to support machine learning model development and contribute to ¬£ value for our theoretical business Utilised K-means clustering to analyse correlations between heart rate from high intensity vs low intensity activities, generating a 14% improvement in heart rate correlation analysis Libraries utilised:\nPandas Numpy Matplotlib Seaborn Sklearn SciPy Development environment:\nJupyter Notebook (Anaconda) Hippo Quest üëë # Implemented object-oriented programming (OOP) to structure the game using classes and objects, improving code maintainability and modularity by 20% Deployed unit testing to minimise game errors and identify any bugs, reducing game errors by 15% Created UML diagram to assist in game and code explanation for interested any parties Utilised debugging to monitor breakpoints and handle fixes Library utilised\nUnittest Development environment:\nPyCharm ","date":null,"permalink":"/projects/","section":"About","summary":"","title":"Projects"},{"content":"Hi everyone, I\u0026rsquo;m Aamir üëã\nI am a Freelance Data Scientist, Writer and Tutor, currently working within London, UK.\nI specialise in is classical machine learning, data analysis and data visualisations to help solve real-world issues.\nMy experience includes:\nComparing the effectiveness supervised and unsupervised such as: XGBoost, logistic regression, Random forest and K-means clustering for fraud classification Optimising classical machine learning models through the use of hyperparameter optimisation methods such as, GridSearchCV Applying a wide variety of data cleansing, extraction and data analysis to health data to identify trends and assist with hypothesis testing for real-world scenarios Created a wide variety of data visualisations to assist with correlation analysis, outlier removal and statistical analysis of health data Education üìö # 2024: Data Science MSc (Distinction) University of Sussex, East Sussex, UK 2022: Economics BSc (2:1) University of Sussex, East Sussex, UK Experience üë®‚Äçüíª # Freelance Data Scientist on Linkedin (Feb 2025 - Present), London, UK Private Tutor on SuperProf (Nov 2024 - Present), London, UK Advisor at University of Sussex Business School (September 2022 - May 2023), Falmer, UK. Editor-in-Chief at Jubilee Times (September 2022 - May 2023), Falmer, UK Intern MI Analyst at The Pensions Regulator (August 2021 - August 2022), Brighton, UK President at Sussex Economics Society (June 2021 - August 2022), Falmer, UK Tech Stack üõ†Ô∏è # Proficient In: Python, SQL, and R Familiar With: HTML and DAX Tools: Jupyter Notebook, PyCharm, LaTeX, Stata, Django, Power BI, Google Colab Awards üèÜ # Gold Spirit of Sussex Award from the University of Sussex Economics Steering Group Award from the University of Sussex Best Speaker Award from Gatwick Communicators ","date":null,"permalink":"/cv/","section":"About","summary":"","title":"Who am I?"}]